{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa79ad-f0cf-4a7f-8d0f-25f1f55a5816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365a4220-f230-4d7d-b460-ae9d88080868",
   "metadata": {},
   "source": [
    "# Unit Test\n",
    "\n",
    "Test whether the implementation is correct, continiuous and efficient. Unit tests automate the repetitive testing process and saves time. A function is tested after the first implementation and then any time the function is modiefied, which happens mainly when new bugs are found, new features are implemeted or the code is refactored. \n",
    "\n",
    "Libraries for unit testing:\n",
    "* pytest\n",
    "* unittest\n",
    "* nosetests\n",
    "* doctests\n",
    "\n",
    "pytest has mainly all functions, is easy to use, and is most popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28772d35-efbe-440d-bdc9-499682b3ca92",
   "metadata": {},
   "source": [
    "Unit tests ...\n",
    "* test the code / time saving\n",
    "* serve as documentation\n",
    "* provide more trust as users can verify the package itself\\\n",
    "* reduce downtime \n",
    "    * e.g. by CI, CI runs all unit tests. 1) If any unit test fails, it reject the change preventing downtime. 2) it infomrs that the code needs to be fixed. \n",
    "    \n",
    "definition of unit:\n",
    "* any small independent piece of code\n",
    "* Python function or class.\n",
    "\n",
    "test types:\n",
    "* unit test: check any small independent piece of code.\n",
    "* integration test: check if units which are dependent work well together. \n",
    "* end-to-end test: check the whole software at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25fe6e-78b0-49ca-9f93-4af005a8edc6",
   "metadata": {},
   "source": [
    "## basics\n",
    "\n",
    "1. Create a file\n",
    "2. import modules\n",
    "3. import function of the test (test_function) \n",
    "4. define a unit test for a specific case -> test wheter the function has the expected return value when called on this particular argument. \n",
    "5. assertion\n",
    "    * boolean_expression -> put in the expected output. True -> return nothing, False -> return AssertionError\n",
    "    * message -> information why the AssertionError was raised. (recommened, more readable to understand)\n",
    "6. Run the unit test\n",
    "7. Read the test result report and spt the bug\n",
    "8. Fix the bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1864f2-0a63-4117-b275-f4ca84799760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "# test module: test_function_to_test.py\n",
    "\n",
    "# step 2: import modules\n",
    "import pytest\n",
    "\n",
    "# step 3: import function to test\n",
    "import function_to_test \n",
    "\n",
    "# step 4: unit test 1\n",
    "def test_for_clean_row():\n",
    "    \n",
    "# step 5: boolean expression\n",
    "    assert function_to_test('possible\\input\\expected') == ['expected', 'output'] \n",
    "    \n",
    "# step 4: unit test 2\n",
    "def test_for_missing_area():\n",
    "    \n",
    "# step 5: boolean expression with message\n",
    "    actual = function_to_test('possible\\input')\n",
    "    expected = None\n",
    "    message = (\"function_to_test('possible\\input') returned {0} instead of {1}\".format(actual, expected))\n",
    "    assert actual is expected, message\n",
    "\n",
    "# step 6: run test\n",
    "!pytest test_function_to_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be4134-8bc7-4947-963f-19dd5ef068d1",
   "metadata": {},
   "source": [
    "#### Multiple assertion in one unit test\n",
    "\n",
    "Unit test can have more than one statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81350462-8b0a-4c11-a2a7-5da7fb52d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "import pytest\n",
    "import convert_to_int\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    return_value = convert_to_int(\"2,081\")\n",
    "    \n",
    "    # Check if expected type is int\n",
    "    assert isinstance(return_value, int)\n",
    "    \n",
    "    # Check if the values matches \n",
    "    assert return_value == 2081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b700ef5-bba1-43f8-b852-3ca0ca882624",
   "metadata": {},
   "source": [
    "### Be aware of floats\n",
    "\n",
    "`0.1 + 0.1 + 0.1 == 0.3` returns always in `False`. So the usual way to control floats does not always work. \n",
    "\n",
    "Use: `pytest.approx()` to wrap expected return value, like: \n",
    "* `assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3)`\n",
    "* `assert np.array([0.1 + 0.1, 0.1 + 0.1, 0.1 + 0.1]) == pytest.approx(np.array([0.2, 0.3]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4a276-735f-4f05-bb1a-29d840a0b3b1",
   "metadata": {},
   "source": [
    "### Folder structure\n",
    "\n",
    "1. Data module:  raw data --> clean data\n",
    "2. Feaute module:  clean data --> features\n",
    "3. Model module: features --> models\n",
    "4. Visualisation module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666890c8-946a-4b5a-b4a7-cce7cee55144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
