{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa79ad-f0cf-4a7f-8d0f-25f1f55a5816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365a4220-f230-4d7d-b460-ae9d88080868",
   "metadata": {},
   "source": [
    "# Unit Test\n",
    "\n",
    "Test whether the implementation is correct, continiuous and efficient. Unit tests automate the repetitive testing process and saves time. A function is tested after the first implementation and then any time the function is modiefied, which happens mainly when new bugs are found, new features are implemeted or the code is refactored. \n",
    "\n",
    "Libraries for unit testing:\n",
    "* pytest\n",
    "* unittest\n",
    "* nosetests\n",
    "* doctests\n",
    "\n",
    "pytest has mainly all functions, is easy to use, and is most popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28772d35-efbe-440d-bdc9-499682b3ca92",
   "metadata": {},
   "source": [
    "Unit tests ...\n",
    "* test the code / time saving\n",
    "* serve as documentation\n",
    "* provide more trust as users can verify the package itself\\\n",
    "* reduce downtime \n",
    "    * e.g. by CI, CI runs all unit tests. 1) If any unit test fails, it reject the change preventing downtime. 2) it infomrs that the code needs to be fixed. \n",
    "    \n",
    "definition of unit:\n",
    "* any small independent piece of code\n",
    "* Python function or class.\n",
    "\n",
    "test types:\n",
    "* unit test: check any small independent piece of code.\n",
    "* integration test: check if units which are dependent work well together. \n",
    "* end-to-end test: check the whole software at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25fe6e-78b0-49ca-9f93-4af005a8edc6",
   "metadata": {},
   "source": [
    "## basics (assertion)\n",
    "\n",
    "1. Create a file\n",
    "2. import modules\n",
    "3. import function of the test (test_function) \n",
    "4. define a unit test for a specific case -> test wheter the function has the expected return value when called on this particular argument. \n",
    "5. assertion \n",
    "    * boolean_expression -> check if the function returnes the expected output. True -> return nothing, False -> return AssertionError\n",
    "    * message -> information why the AssertionError was raised. (recommened, more readable to understand)\n",
    "6. Run the unit test\n",
    "7. Read the test result report and spt the bug\n",
    "8. Fix the bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1864f2-0a63-4117-b275-f4ca84799760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "# test module: test_function_to_test.py\n",
    "\n",
    "# step 2: import modules\n",
    "import pytest\n",
    "\n",
    "# step 3: import function to test\n",
    "import function_to_test \n",
    "\n",
    "# step 4: unit test 1\n",
    "def test_for_clean_row():\n",
    "    \n",
    "# step 5: boolean expression\n",
    "    assert function_to_test('possible\\input\\expected') == ['expected', 'output'] \n",
    "    \n",
    "# step 4: unit test 2\n",
    "def test_for_missing_area():\n",
    "    \n",
    "# step 5: boolean expression with message\n",
    "    actual = function_to_test('possible\\input')\n",
    "    expected = None\n",
    "    message = (\"function_to_test('possible\\input') returned {0} instead of {1}\".format(actual, expected))\n",
    "    assert actual is expected, message\n",
    "\n",
    "# step 6: run test\n",
    "!pytest test_function_to_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be4134-8bc7-4947-963f-19dd5ef068d1",
   "metadata": {},
   "source": [
    "#### Multiple assertion in one unit test\n",
    "\n",
    "Unit test can have more than one statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81350462-8b0a-4c11-a2a7-5da7fb52d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "import pytest\n",
    "import convert_to_int\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    return_value = convert_to_int(\"2,081\")\n",
    "    \n",
    "    # Check if expected type is int\n",
    "    assert isinstance(return_value, int)\n",
    "    \n",
    "    # Check if the values matches \n",
    "    assert return_value == 2081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b700ef5-bba1-43f8-b852-3ca0ca882624",
   "metadata": {},
   "source": [
    "### Be aware of floats\n",
    "\n",
    "`0.1 + 0.1 + 0.1 == 0.3` returns always in `False`. So the usual way to control floats does not always work. \n",
    "\n",
    "Use: `pytest.approx()` to wrap expected return value, like: \n",
    "* `assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3)`\n",
    "* `assert np.array([0.1 + 0.1, 0.1 + 0.1, 0.1 + 0.1]) == pytest.approx(np.array([0.2, 0.3]))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9969acc-487d-4d75-835e-0edad3aa73a2",
   "metadata": {},
   "source": [
    "## basics (exceptions) \n",
    "\n",
    "Test if a function raises the expected error\n",
    "1. Create a file\n",
    "2. import modules\n",
    "3. import function of the test (test_function) \n",
    "4. define a unit test for a specific case -> test wheter the function has the expected return value when called on this particular argument. \n",
    "5. raise exception \n",
    "    * ...\n",
    "6. Run the unit test\n",
    "7. Read the test result report and spt the bug\n",
    "8. Fix the bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0d4e4-da05-420c-9263-da2556b2724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "# test module: test_function_to_test.py\n",
    "\n",
    "# step 2: import modules\n",
    "import pytest\n",
    "\n",
    "# step 3: import function to test\n",
    "import function_to_test \n",
    "\n",
    "\n",
    "# step 4: unit test 1\n",
    "def test_valueerror_on_one_dimensional_array():\n",
    "    \n",
    "# step 5: raise exception\n",
    "    example_argument = np.array([2081, 315145, 1059, 186606, 1257, 206186])\n",
    "    \n",
    "    with pytest.raises(ValueError):\n",
    "        function_to_test(example_argument)\n",
    "        \n",
    "        # if the function raises expected ValueError, test will pass\n",
    "        # if the function does not raise a ValueError, test will fail\n",
    "        \n",
    "        \n",
    "# step 4: unit test 2\n",
    "def test_valueerror_on_one_dimensional_array_2():\n",
    "    \n",
    "# step 5: raise exception with message\n",
    "    example_argument_2 = np.array([2081, 315145, 1059, 186606, 1257, 206186])\n",
    "    \n",
    "    with pytest.raises(ValueError) as exception_info:\n",
    "        function_to_test(example_argument_2)\n",
    "        \n",
    "    # Check if ValueError contains the correct message\n",
    "    assert exeption_info.match(\"Argument data array must be two dimentional.\" \"Got 1 dimensional array instead!\")\n",
    "\n",
    "    \n",
    "# step 6: run test\n",
    "!pytest test_function_to_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f843526-cfe3-4f8b-b482-5f84f0ad158c",
   "metadata": {},
   "source": [
    "#### with-function explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b4844-e7f7-4d92-9c9f-94d4e54c2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exception is silienced    \n",
    "    with pytest.raises(ValueError):\n",
    "        raise ValueError # context exits with ValueError\n",
    "        # <-- pytest.raises(ValueError) silences it\n",
    "        \n",
    "# exception raised as failed\n",
    "    with pytest.raises(ValueError):\n",
    "        pass # context exits without raising a ValueError\n",
    "        # <-- pytest.raises(ValueError) raises Failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6511648-555d-42b3-b9e0-34c124659a25",
   "metadata": {},
   "source": [
    "### How many tests are considered enough?\n",
    "\n",
    "If we have tested for all of the following argument types, then the function can be decalered well tested.\n",
    "1. Bad arguments \n",
    "    * When passed bad arguments, function reaises an error \n",
    "2. Special arguments\n",
    "    * Boundary values: points that mark separating the normal behaviour and special behaviour. \n",
    "    * For some argument values, the funcion uses special logic.  \n",
    "3. Normal arguments\n",
    "    * Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4a276-735f-4f05-bb1a-29d840a0b3b1",
   "metadata": {},
   "source": [
    "### Folder structure\n",
    "\n",
    "1. Data module:  raw data --> clean data\n",
    "2. Feaute module:  clean data --> features\n",
    "3. Model module: features --> models\n",
    "4. Visualisation module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666890c8-946a-4b5a-b4a7-cce7cee55144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
