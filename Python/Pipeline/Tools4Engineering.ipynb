{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1462b45d",
   "metadata": {},
   "source": [
    "# Introduction into Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd347391",
   "metadata": {},
   "source": [
    "## Existing tools\n",
    "Data bases\n",
    "* MySQL\n",
    "* PostegreSQL\n",
    "\n",
    "Processing\n",
    "* Apache Spark (uses DataFrame abstraction, language: PySpark, Py, Scala)\n",
    "* Hadoop (structure queries for parallel processing)\n",
    "    * HDFS\n",
    "    * MapReduce\n",
    "* Hive (runs on Hadoop MapReduce, is SQL)\n",
    "\n",
    "Scheduling\n",
    "* (Linux's `cron`)\n",
    "* (Spotrify's Luigi)\n",
    "* Apache AirFlow\n",
    "* Oozie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE IN PYSPARK\n",
    "\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd7765",
   "metadata": {},
   "source": [
    "## Processing\n",
    "* Clean data\n",
    "* Aggregate data\n",
    "* Join data\n",
    "\n",
    "Data processing must be distributed over clusters of virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401fe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "multiprocessing.Pool\n",
    "\n",
    "def take_mean_age(year_and_group):\n",
    "    year, group = year_and_group\n",
    "    return pd.DataFrame({\"Age\": group['Age'].mean()}, index = [year])\n",
    "\n",
    "with Pool(4) as p:\n",
    "    results = p.map(take_mean_age, athlete_events.groupby('Year'))\n",
    "    \n",
    "results_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e63e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Partition datafrom into 4\n",
    "event_dask = dd.from_pandas(athlete_evetns, npartitions = 4)\n",
    "\n",
    "# Run parallel compuations on each partition\n",
    "results_df = event_dask.groupby('Year').Age.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557dcf1a",
   "metadata": {},
   "source": [
    "## Scheduling\n",
    "* Plan jobs with specific time intervals\n",
    "* Resolve dependency requirements of jobs\n",
    "\n",
    "Tasks to make sure:\n",
    "- Jobs run in a specific order and all dependencies are resolved correctly\n",
    "- Jobs run at a specific time each day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87049569",
   "metadata": {},
   "source": [
    "For scheduling events, you can use Direct Acyclic Graph (DAGs):\n",
    "* Set of nodes\n",
    "* Directed edges\n",
    "* No cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0014f71",
   "metadata": {},
   "source": [
    "Recommanded tool: Apache Airflow\n",
    "* Created at Airbnb\n",
    "* Uses DAGs\n",
    "* Uses Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operators import PythonOperator\n",
    "\n",
    "# Create a DAG\n",
    "dag = DAG(dag_id = 'example_dag', ..., scheduling_interval = '0 * * * *') # Every hour at minute 0 \n",
    "# minute [0-59], hour [0-23], monthday [1-31], month [1-12], weekday [0-6]\n",
    "\n",
    "# Define operations\n",
    "start_cluster = StartClusterOperator(task_id = 'start_cluster')\n",
    "ingest_customer_data = SparkJobOperator(task_id = 'ingest_customer_data' , dag = dag)\n",
    "ingest_product_data = SparkJobOperator(task_id = 'ingest_product_data' , dag = dag)\n",
    "enrich_customer_data = PythonOperator(task_id = 'enrich_customer_data' , dag = dag) \n",
    "    # python_callable = <function ETL process>\n",
    "    # op_kwargs = {\"argument\": argument_var}\n",
    "\n",
    "# Set up dependency flow [set_upstream. set_downstream]\n",
    "start_cluster.set_downstram(ingest_customer_data)\n",
    "ingest_customer_data.set_downsteam(enrich_customer_data)\n",
    "ingest_product_data.set_downsteam(enrich_customer_data)\n",
    "\n",
    "# Save the file.py in ~/airflow/dags/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f534f",
   "metadata": {},
   "source": [
    "### Storage locaties\n",
    "\n",
    "Data should be stored at different locations, for hackers and destruction of the datacenter.\n",
    "\n",
    "* Azure\n",
    "* Google cloud\n",
    "* Amazon web services\n",
    "\n",
    "Storage\n",
    "- Azure: Azure Blob Storage\n",
    "- Google: Google Cloud Storage\n",
    "- Amazon: AWS3\n",
    "\n",
    "Compute \n",
    "- Azure: Azure Virual Machines\n",
    "- Google: Google Compute Engine\n",
    "- Amazon: SWS EC2\n",
    "\n",
    "Databases\n",
    "- Azure: Azure SQL database\n",
    "- Google: Google Cloud SQL\n",
    "- Amazon: AWS RDS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7286a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
